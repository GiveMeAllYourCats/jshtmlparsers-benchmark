"use strict";
/*!
 *
 * Copyright 2017 - acrazing
 *
 * @author acrazing joking.young@gmail.com
 * @since 2017-08-19 14:15:25
 * @version 1.0.0
 * @desc tokenize.spec.ts
 */
Object.defineProperty(exports, "__esModule", { value: true });
var tslib_1 = require("tslib");
var assert = tslib_1.__importStar(require("assert"));
var tokenize_1 = require("./tokenize");
var index = 0;
function token(value, type, start) {
    if (type === void 0) { type = tokenize_1.TokenKind.Literal; }
    if (start === void 0) { start = index; }
    var v = {
        start: start,
        end: start + value.length,
        value: value,
        type: type,
    };
    index = v.end;
    return v;
}
var cases = [
    {
        name: 'single Literal',
        input: 'hello',
        tokens: [token('hello', tokenize_1.TokenKind.Literal, 0)],
    },
    {
        name: 'Literal end with <',
        input: 'hello<',
        tokens: [token('hello', void 0, 0), token('<')],
    },
    {
        name: 'Literal unexpected <',
        input: 'hello< world',
        tokens: [token('hello', void 0, 0), token('< world')],
    },
    {
        name: 'OpenTag EOF',
        input: '<div',
        tokens: [token('div', tokenize_1.TokenKind.OpenTag, 1)],
    },
    {
        name: 'attribute names',
        input: "<div a1 'b2' \"c3\" 'd4'e5 'f6\"' \"g7'\"></div>",
        tokens: [
            token('div', tokenize_1.TokenKind.OpenTag, 1),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('a1', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token("'b2'", tokenize_1.TokenKind.AttrValueSq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('"c3"', tokenize_1.TokenKind.AttrValueDq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token("'d4'", tokenize_1.TokenKind.AttrValueSq),
            token('e5', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token("'f6\"'", tokenize_1.TokenKind.AttrValueSq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('"g7\'"', tokenize_1.TokenKind.AttrValueDq),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('div', tokenize_1.TokenKind.CloseTag, index + 3),
        ],
    },
    {
        name: 'attribute values',
        input: '<div a b= c=1 d e = f = g \'h\'=i "j"k=lmn o=\'pq\' r="st"u>M</div>',
        tokens: [
            token('div', tokenize_1.TokenKind.OpenTag, 1),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('a', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('b', tokenize_1.TokenKind.AttrValueNq),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('c', tokenize_1.TokenKind.AttrValueNq),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token('1', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('d', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('e', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('f', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('g', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token("'h'", tokenize_1.TokenKind.AttrValueSq),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token('i', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('"j"', tokenize_1.TokenKind.AttrValueDq),
            token('k', tokenize_1.TokenKind.AttrValueNq),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token('lmn', tokenize_1.TokenKind.AttrValueNq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('o', tokenize_1.TokenKind.AttrValueNq),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token("'pq'", tokenize_1.TokenKind.AttrValueSq),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('r', tokenize_1.TokenKind.AttrValueNq),
            token('=', tokenize_1.TokenKind.AttrValueEq),
            token('"st"', tokenize_1.TokenKind.AttrValueDq),
            token('u', tokenize_1.TokenKind.AttrValueNq),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('M', void 0, index + 1),
            token('div', tokenize_1.TokenKind.CloseTag, index + 2),
        ],
    },
    {
        name: 'normal doctype',
        input: '<!doctype html>',
        tokens: [
            token('!doctype', tokenize_1.TokenKind.OpenTag, 1),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('html', tokenize_1.TokenKind.AttrValueNq),
            token('', tokenize_1.TokenKind.OpenTagEnd),
        ],
    },
    {
        name: 'unexpected eof end doctype',
        input: '<!doctype',
        tokens: [token('!doctype', tokenize_1.TokenKind.OpenTag, 1)],
    },
    {
        name: 'unexpected eof in doctype',
        input: '<!doctyp',
        tokens: [token('!', tokenize_1.TokenKind.OpenTag, 1), token('doctyp')],
    },
    {
        name: 'normal comment',
        input: '<!-- hello world -->',
        tokens: [
            token('!--', tokenize_1.TokenKind.OpenTag, 1),
            token(' hello world '),
            token('--', tokenize_1.TokenKind.OpenTagEnd),
        ],
    },
    {
        name: 'short comment',
        input: '<? hello world ?><!- hello world ->',
        tokens: [
            token('', tokenize_1.TokenKind.OpenTag, 1),
            token('? hello world ?'),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('!', tokenize_1.TokenKind.OpenTag, index + 2),
            token('- hello world -'),
            token('', tokenize_1.TokenKind.OpenTagEnd),
        ],
    },
    {
        name: 'open tag end',
        input: '<a1><b2/><c3 /><d4  /   ><e5    f6/><g7     /h8><i9      /j10/><k11//>',
        tokens: [
            token('a1', tokenize_1.TokenKind.OpenTag, 1),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('b2', tokenize_1.TokenKind.OpenTag, index + 2),
            token('/', tokenize_1.TokenKind.OpenTagEnd),
            token('c3', tokenize_1.TokenKind.OpenTag, index + 2),
            token(' ', tokenize_1.TokenKind.Whitespace),
            token('/', tokenize_1.TokenKind.OpenTagEnd),
            token('d4', tokenize_1.TokenKind.OpenTag, index + 2),
            token('  ', tokenize_1.TokenKind.Whitespace),
            token('/', tokenize_1.TokenKind.AttrValueNq),
            token('   ', tokenize_1.TokenKind.Whitespace),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('e5', tokenize_1.TokenKind.OpenTag, index + 2),
            token('    ', tokenize_1.TokenKind.Whitespace),
            token('f6', tokenize_1.TokenKind.AttrValueNq),
            token('/', tokenize_1.TokenKind.OpenTagEnd),
            token('g7', tokenize_1.TokenKind.OpenTag, index + 2),
            token('     ', tokenize_1.TokenKind.Whitespace),
            token('/', tokenize_1.TokenKind.AttrValueNq),
            token('h8', tokenize_1.TokenKind.AttrValueNq),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('i9', tokenize_1.TokenKind.OpenTag, index + 2),
            token('      ', tokenize_1.TokenKind.Whitespace),
            token('/', tokenize_1.TokenKind.AttrValueNq),
            token('j10', tokenize_1.TokenKind.AttrValueNq),
            token('/', tokenize_1.TokenKind.OpenTagEnd),
            token('k11', tokenize_1.TokenKind.OpenTag, index + 2),
            token('/', tokenize_1.TokenKind.AttrValueNq),
            token('/', tokenize_1.TokenKind.OpenTagEnd),
        ],
    },
    {
        name: 'close tag',
        input: '</div></ div >',
        tokens: [
            token('div', tokenize_1.TokenKind.CloseTag, 2),
            token(' div ', tokenize_1.TokenKind.CloseTag, index + 3),
        ],
    },
    {
        name: 'special normal comment',
        input: '<!---- - -- ---->',
        tokens: [
            token('!--', tokenize_1.TokenKind.OpenTag, 1),
            token('-- '),
            token('- '),
            token('-- '),
            token('-'),
            token('-'),
            token('--', tokenize_1.TokenKind.OpenTagEnd),
        ],
    },
    {
        name: 'script',
        input: '<script></div></script</script >',
        tokens: [
            token('script', tokenize_1.TokenKind.OpenTag, 1),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('</div>', tokenize_1.TokenKind.Literal, index + 1),
            token('</script'),
            token('script ', tokenize_1.TokenKind.CloseTag, index + 2),
        ],
    },
    {
        name: 'style',
        input: '<style></div></style</style >',
        tokens: [
            token('style', tokenize_1.TokenKind.OpenTag, 1),
            token('', tokenize_1.TokenKind.OpenTagEnd),
            token('</div>', tokenize_1.TokenKind.Literal, index + 1),
            token('</style'),
            token('style ', tokenize_1.TokenKind.CloseTag, index + 2),
        ],
    },
];
describe('simple cases', function () {
    var e_1, _a;
    var _loop_1 = function (_case) {
        it("case \"" + _case.name + "\"", function () {
            var tokens = tokenize_1.tokenize(_case.input);
            assert.deepStrictEqual(tokens, _case.tokens);
        });
    };
    try {
        for (var cases_1 = tslib_1.__values(cases), cases_1_1 = cases_1.next(); !cases_1_1.done; cases_1_1 = cases_1.next()) {
            var _case = cases_1_1.value;
            _loop_1(_case);
        }
    }
    catch (e_1_1) { e_1 = { error: e_1_1 }; }
    finally {
        try {
            if (cases_1_1 && !cases_1_1.done && (_a = cases_1.return)) _a.call(cases_1);
        }
        finally { if (e_1) throw e_1.error; }
    }
});
//# sourceMappingURL=tokenize.spec.js.map